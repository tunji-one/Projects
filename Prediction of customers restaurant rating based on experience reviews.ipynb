{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = pd.read_csv('yelp.csv')\n",
    "rev.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = pd.DatetimeIndex(rev['date'])\n",
    "rev['year'] =  date.year\n",
    "rev['month'] = date.month\n",
    "rev['day'] = date.strftime('%a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(rev['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = rev[['business_id','user_id', 'date','text','year','month','day','stars']]\n",
    "c1 = reviews['stars'] == 5\n",
    "c2 = reviews['stars'] == 2\n",
    "reviews = reviews[c1|c2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split                  #split the data 'reviews' into train & test\n",
    "train, test= train_test_split(reviews, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sort_values('date')    #got the target class and text\n",
    "y_train = train['stars']\n",
    "X_train = train['text']\n",
    "\n",
    "y_test = test['stars']\n",
    "X_test = test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_X_train = train[['business_id', 'user_id', 'day']]  #put the other categorical columns in another dataframe\n",
    "                                                        #for target encoding\n",
    "sub_X_test  = test[['business_id', 'user_id', 'day']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce                             \n",
    "def cat_trans(a,b,c):                                   #function for target encoding\n",
    "    cat_feat = ['business_id', 'user_id', 'day']\n",
    "    t_encoder = ce.TargetEncoder(cols = cat_feat)\n",
    "    t_encoder.fit(a, c)\n",
    "    sub_X_train = t_encoder.transform(a)\n",
    "    sub_X_test = t_encoder.transform(b)\n",
    "    return (sub_X_train, sub_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = cat_trans(sub_X_train, sub_X_test, y_train)   #retrieving the results of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lema = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_a_text(text):                               #function for cleaning the text\n",
    "    split_sentence = nltk.word_tokenize(text)\n",
    "    no_punc = [char.lower() for char in split_sentence if char not in string.punctuation]\n",
    "    sw_removed = [word for word in no_punc if word not in stopwords]\n",
    "    lemmatized = [lema.lemmatize(word, pos = wordnet.VERB) for word in sw_removed]\n",
    "    pattern = \"[^~''`/*0-9... :]+\"\n",
    "    patt = '[a-zA-Z][a-zA-Z]+'\n",
    "    final = re.findall(pattern, ' '.join(lemmatized))\n",
    "    final2 = re.findall(patt, ' '.join(final))\n",
    "    return final2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "                                                    #function for plotting word cloud of all words in the text\n",
    "def plot_word_cloud(text):\n",
    "    wordcloud_instance = WordCloud(width = 1500, height = 900, \n",
    "                background_color ='black', \n",
    "                stopwords=None,\n",
    "                min_font_size = 10).generate(text)\n",
    "    \n",
    "    plt.figure(figsize = (10, 8), facecolor = None) \n",
    "    plt.imshow(wordcloud_instance) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ''                                             #combine all cleaned words from text into a string\n",
    "for index, row in reviews[['text']].iterrows():\n",
    "    for each in ' '.join(clean_a_text(row['text'])):\n",
    "        words = words + each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_cloud(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_vector = CountVectorizer(analyzer= clean_a_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_vector.fit(train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_values = words_vector.transform(train['text'])\n",
    "X_test_values = words_vector.transform(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_values.shape, X_test_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_values = tfidf.fit_transform(X_train_values)\n",
    "X_test_values = tfidf.fit_transform(X_test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = words_vector.get_feature_names()   #got each word derived from the CountVectorizer analyzer as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_after_tfidf = pd.DataFrame(X_train_values.toarray(), columns= features, index= train.index) #dataframe of values after\n",
    "                                                                                                    #countvectorizer and tfidf\n",
    "X_text_after_tfidf = pd.DataFrame(X_test_values.toarray(), columns= features, index= test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_m = train[['year', 'month']]     #numeric dataframe to be concatenated with tfidf and target encoded dataframe\n",
    "test_y_m = test[['year', 'month']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([a[0], train_y_m , X_train_after_tfidf], axis = 1)  #final dataframe\n",
    "\n",
    "X_test = pd.concat([a[1], test_y_m , X_text_after_tfidf], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipeline = make_pipeline(SMOTE(),     \n",
    "                        MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipeline.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = my_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
